{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "450380bf",
   "metadata": {},
   "source": [
    "# Pre "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13493e27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T08:10:31.229773Z",
     "start_time": "2024-03-12T08:10:30.629541Z"
    },
    "code_folding": [
     57,
     68,
     79,
     94,
     110,
     114,
     138,
     159,
     163,
     164,
     184
    ]
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pdb\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from skmultilearn.ensemble import RakelD\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from skmultilearn.adapt import BRkNNbClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from skmultilearn.dataset import load_dataset\n",
    "from skmultilearn.dataset import load_from_arff\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from skmultilearn.ensemble import RakelO\n",
    "from skmultilearn.adapt import MLTSVM\n",
    "from skmultilearn.adapt import MLARAM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from enum import Enum\n",
    "import sklearn.metrics as metrics\n",
    "from scipy import sparse\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from skmultilearn.dataset import load_dataset\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import sklearn.metrics as metrics\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from skmultilearn.dataset import load_from_arff\n",
    "from skmultilearn.ensemble import RakelD\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import model_selection, preprocessing\n",
    "import scipy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Imbalance and need function\n",
    "\n",
    "def CardAndDens(X,y):\n",
    "    cardmatrix=[]\n",
    "    for i in range(X.shape[0]):\n",
    "        count=0\n",
    "        for j in range(y.shape[1]):\n",
    "            if y[i,j]==1:\n",
    "                count+=1\n",
    "        cardmatrix.append(count)\n",
    "    Card=sum(cardmatrix)/len(cardmatrix)\n",
    "    Dens=Card/y.shape[1]\n",
    "    return Card,Dens\n",
    "def FeatureSelect(p):\n",
    "    if p==1:\n",
    "        return X.toarray(),feature_names\n",
    "    else:\n",
    "        featurecount=int(X.shape[1]*p)\n",
    "        Selectfeatureindex=[x[0] for x in (sorted(enumerate(X.sum(axis=0).tolist()[0]),key=lambda x: x[1],reverse=True))][:featurecount]\n",
    "        Allfeatureindex=[i for i in range(X.shape[1])]\n",
    "        featureindex=[i for i in Allfeatureindex if i not in Selectfeatureindex]\n",
    "        new_x=np.delete(X.toarray(),featureindex,axis=1)\n",
    "        new_featurename=[feature_names[i] for i in Selectfeatureindex] \n",
    "        return new_x,new_featurename\n",
    "def ImR(X,y):\n",
    "    Imr=[]\n",
    "    for i in range(y.shape[1]):\n",
    "        count0=0\n",
    "        count1=0\n",
    "        for j in range(y.shape[0]):\n",
    "            if y[j,i]==1:\n",
    "                count1+=1\n",
    "            else:\n",
    "                count0+=1\n",
    "        if count1<=count0:\n",
    "            Imr.append(count0/count1)\n",
    "        else:\n",
    "            Imr.append(count1/count0)\n",
    "    return Imr\n",
    "def Imbalance(X,y):\n",
    "    countmatrix=[]\n",
    "    for i in range(y.shape[1]):\n",
    "        count0=0\n",
    "        count1=0\n",
    "        for j in range(y.shape[0]):\n",
    "            if y[j,i]==1:\n",
    "                count1+=1\n",
    "            else:\n",
    "                count0+=1\n",
    "        countmatrix.append(count1)\n",
    "    maxcount=max(countmatrix)\n",
    "    ImbalanceRatioMatrix=[maxcount/i for i in countmatrix]\n",
    "    MaxIR=max(ImbalanceRatioMatrix)\n",
    "    MeanIR=sum(ImbalanceRatioMatrix)/len(ImbalanceRatioMatrix)\n",
    "    return ImbalanceRatioMatrix,MeanIR,countmatrix\n",
    "def Scumble(X,y):\n",
    "    ImbalanceRatioMatrix,MeanIR,_=Imbalance(X,y)\n",
    "    DifferenceImbalanceRatioMatrix=[i-MeanIR for i in ImbalanceRatioMatrix]\n",
    "    count=0\n",
    "    for i in range(y.shape[1]):\n",
    "        count+=math.pow(DifferenceImbalanceRatioMatrix[i],2)/(y.shape[1]-1)\n",
    "    ImbalanceRatioSigma=math.sqrt(count)\n",
    "    CVIR=ImbalanceRatioSigma/MeanIR\n",
    "    SumScumble=0\n",
    "    Scumble_i=[]\n",
    "    for i in range(y.shape[0]):\n",
    "        count=0\n",
    "        prod=1\n",
    "        SumIRLbl=0\n",
    "        for j in range(y.shape[1]):\n",
    "            IRLbl=1\n",
    "            if y[i,j]==1:\n",
    "                IRLbl=ImbalanceRatioMatrix[j]\n",
    "                SumIRLbl+=IRLbl\n",
    "                prod*=IRLbl\n",
    "                count+=1\n",
    "        if count==0:\n",
    "            Scumble_i.append(0)\n",
    "        else:\n",
    "            IRLbl_i=SumIRLbl/count\n",
    "            Scumble_i.append(1.0-((1.0/IRLbl_i) * math.pow(prod, 1.0/count)))\n",
    "    scumble=sum(Scumble_i)/X.shape[0]\n",
    "    return Scumble_i,scumble,CVIR\n",
    "def Labeltype(X,y):\n",
    "    ImbalanceRatioMatrix,MeanIR,_=Imbalance(X,y)\n",
    "    DifferenceImbalanceRatioMatrix=[i-MeanIR for i in ImbalanceRatioMatrix]\n",
    "    MinLabelIndex=[]\n",
    "    MajLabelIndex=[]\n",
    "    count=0\n",
    "    for i in (DifferenceImbalanceRatioMatrix):\n",
    "        if i>0:\n",
    "            MinLabelIndex.append(count)\n",
    "        else:\n",
    "            MajLabelIndex.append(count)\n",
    "        count+=1\n",
    "    MinLabelName=[]\n",
    "    MajLabelName=[]\n",
    "    for i in MinLabelIndex:\n",
    "        MinLabelName.append(label_names[i][0])\n",
    "    for i in MajLabelIndex:\n",
    "        MajLabelName.append(label_names[i][0])\n",
    "    MinLabeldic=dict(zip(MinLabelIndex,MinLabelName))\n",
    "    MajLabeldic=dict(zip(MajLabelIndex,MajLabelName))\n",
    "    return MinLabeldic,MajLabeldic\n",
    "def CalcuNN(df1,n_neighbor):\n",
    "    nbs=NearestNeighbors(n_neighbors=n_neighbor,metric='euclidean',algorithm='kd_tree').fit(df1)\n",
    "    euclidean,indices= nbs.kneighbors(df1)\n",
    "    return euclidean,indices\n",
    "def FeatureSelect(p,X):\n",
    "    if p==1:\n",
    "        return X.toarray(),feature_names\n",
    "    else:\n",
    "        if feature_names[1][1]=='NUMERIC':\n",
    "            featurecount=int(X.shape[1]*p)\n",
    "            column_variances = np.var(X.toarray(), axis=0)\n",
    "            sorted_indices = column_variances.argsort()[::-1]\n",
    "            Selectfeatureindex = sorted_indices[:featurecount]\n",
    "            Allfeatureindex=[i for i in range(X.shape[1])]\n",
    "            featureindex=[i for i in Allfeatureindex if i not in Selectfeatureindex]\n",
    "            new_x=np.delete(X.toarray(),featureindex,axis=1)\n",
    "            new_featurename=[feature_names[i] for i in Selectfeatureindex]          \n",
    "        else:\n",
    "            featurecount=int(X.shape[1]*p)\n",
    "            Selectfeatureindex=[x[0] for x in (sorted(enumerate(X.sum(axis=0).tolist()[0]),key=lambda x: x[1],reverse=True))][:featurecount]\n",
    "            Allfeatureindex=[i for i in range(X.shape[1])]\n",
    "            featureindex=[i for i in Allfeatureindex if i not in Selectfeatureindex]\n",
    "            new_x=np.delete(X.toarray(),featureindex,axis=1)\n",
    "            new_featurename=[feature_names[i] for i in Selectfeatureindex] \n",
    "        return new_x,new_featurename\n",
    "def LabelSelect(y):\n",
    "    b=[]\n",
    "    new_labelname=[i for i in label_names]\n",
    "    for i in range(y.shape[1]):\n",
    "        if y[:,i].sum()<=20:\n",
    "            b.append(i)\n",
    "            new_labelname.remove(label_names[i])\n",
    "    new_y=np.delete(y.toarray(),b,axis=1)\n",
    "    return new_y,new_labelname "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025ddd65",
   "metadata": {},
   "source": [
    "# GET W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6551ec8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T08:14:32.174528Z",
     "start_time": "2024-03-12T08:14:32.159210Z"
    },
    "code_folding": [
     1,
     64
    ]
   },
   "outputs": [],
   "source": [
    "# Caculate W\n",
    "def GetW(X, Y, optmParameter):\n",
    "    alpha = optmParameter['alpha']\n",
    "    beta = optmParameter['beta']\n",
    "    gamma = optmParameter['gamma']\n",
    "    lamda = optmParameter['lamda']\n",
    "    lamda2 = optmParameter['lamda2']\n",
    "    maxIter = optmParameter['maxIter']\n",
    "    miniLossMargin = optmParameter['minimumLossMargin']\n",
    "    \n",
    "    # initialization\n",
    "    num_dim = X.shape[1]\n",
    "    XT = X.T\n",
    "    XTX = np.dot(XT, X)\n",
    "    XTY = np.dot(XT, Y)\n",
    "    W_s = np.linalg.solve(XTX + gamma*np.eye(num_dim), XTY)\n",
    "    W_s_1 = W_s\n",
    "    \n",
    "    # label correlation\n",
    "    R = pdist(Y.T+np.finfo(float).eps, metric='cosine')\n",
    "    R = 1 - squareform(R)\n",
    "    C = R.reshape(Y.shape[1], Y.shape[1])\n",
    "    L1 = np.diag(np.sum(C, axis=1)) - C\n",
    "    \n",
    "    S = ins_similarity(X, 5)\n",
    "    L2 = np.diag(np.sum(S, axis=1)) - S\n",
    "    iter = 1\n",
    "    oldloss = 0\n",
    "    bk = 1\n",
    "    bk_1 = 1\n",
    "    \n",
    "    # compute LIP\n",
    "    A = gradL21(W_s)\n",
    "    Lip = np.sqrt(4*(np.linalg.norm(XTX)**2 + np.linalg.norm(alpha*XTX)**2 * np.linalg.norm(L1)**2))   \n",
    "    # proximal gradient\n",
    "    while iter <= maxIter:\n",
    "        A = gradL21(W_s)\n",
    "        W_s_k = W_s + (bk_1 - 1)/bk * (W_s - W_s_1)\n",
    "        gradF = np.dot(XTX, W_s_k) - XTY + alpha * np.dot(XTX, W_s_k).dot(L1) \n",
    "        Gw_s_k = W_s_k - 1/Lip *(gradF)\n",
    "        # update b, W\n",
    "        bk_1 = bk\n",
    "        bk = (1 + np.sqrt(4*bk**2 + 1))/2\n",
    "        W_s_1 = W_s\n",
    "        W_s = softthres(Gw_s_k, beta/Lip)  \n",
    "        # compute loss function\n",
    "        predictionLoss = np.trace(np.dot(X.dot(W_s) - Y, (X.dot(W_s) - Y).T))\n",
    "        F = X.dot(W_s)\n",
    "        correlation = np.trace(F.dot(L1).dot(F.T))\n",
    "        In_correlation = np.trace(F.T.dot(L2).dot(F))\n",
    "        sparsity = np.sum(W_s != 0)\n",
    "        sparsity2 = np.trace(np.dot(W_s.T, A).dot(W_s))\n",
    "        totalloss = predictionLoss + alpha*correlation + beta*sparsity \n",
    "         #  totalloss = predictionLoss + alpha*correlation + beta*sparsity + lamda*In_correlation + lamda2*sparsity2\n",
    "        if abs(oldloss - totalloss) <= miniLossMargin or totalloss <= 0:\n",
    "            break\n",
    "        else:\n",
    "            oldloss = totalloss\n",
    "        iter += 1\n",
    "    \n",
    "    W = W_s\n",
    "    return W\n",
    "def softthres(W_t, lambda_):\n",
    "    return np.maximum(W_t-lambda_, 0) - np.maximum(-W_t-lambda_, 0)\n",
    "def ins_similarity(X, K):\n",
    "    A = squareform(pdist(X))\n",
    "    num_dim = A.shape[0]\n",
    "    for i in range(num_dim):\n",
    "        temp = A[i,:]\n",
    "        As = np.sort(temp)\n",
    "        temp = (temp <= As[K])\n",
    "        A[i,:] = temp\n",
    "    return A\n",
    "def label_similarity(Y,k):\n",
    "    m = Y.shape[1]\n",
    "    cos_sim = np.zeros((m, m))\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            if i == j:\n",
    "                cos_sim[i, j] = 1.0\n",
    "            else:\n",
    "                cos_sim[i, j] = np.dot(Y[:, i], Y[:, j]) / (np.linalg.norm(Y[:, i]) * np.linalg.norm(Y[:, j]))\n",
    "    upper_triangle_indices = np.triu_indices(cos_sim.shape[1], k=1)\n",
    "    upper_triangle_values = cos_sim [upper_triangle_indices]\n",
    "    top_k_indices = np.argpartition(upper_triangle_values, -k)[-k:]\n",
    "    result_matrix = np.zeros_like(cos_sim)\n",
    "    result_matrix[upper_triangle_indices[0][top_k_indices], upper_triangle_indices[1][top_k_indices]] = upper_triangle_values[top_k_indices]\n",
    "    return result_matrix\n",
    "def gradL21(W):\n",
    "    num = W.shape[0]\n",
    "    D = np.zeros((num, num))\n",
    "    for i in range(num):\n",
    "        temp = np.linalg.norm(W[i,:], 2)\n",
    "        if temp != 0:\n",
    "            D[i,i] = 1/temp\n",
    "        else:\n",
    "            D[i,i] = 0\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e875668",
   "metadata": {},
   "source": [
    "# MLC METRIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44465de1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T08:10:32.216149Z",
     "start_time": "2024-03-12T08:10:32.204262Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def macro_averaging_auc(Y, P, O):\n",
    "    n = (Y.shape[0] + O.shape[0]) // 2\n",
    "    l = (Y.shape[1] + O.shape[1]) // 2\n",
    "\n",
    "    p = np.zeros(l)\n",
    "    q = np.sum(Y, 0)\n",
    "\n",
    "    zero_column_count = np.sum(q == 0)\n",
    "#     print(f\"all zero for label: {zero_column_count}\")\n",
    "    r, c = np.nonzero(Y)\n",
    "    for i, j in zip(r, c):\n",
    "        p[j] += np.sum((Y[ : , j] < 0.5) * (O[ : , j] <= O[i, j]))\n",
    "\n",
    "    i = (q > 0) * (q < n)\n",
    "\n",
    "    return np.sum(p[i] / (q[i] * (n - q[i]))) / l\n",
    "def hamming_loss(Y, P, O):\n",
    "    n = (Y.shape[0] + P.shape[0]) // 2\n",
    "    l = (Y.shape[1] + P.shape[1]) // 2\n",
    "\n",
    "    s1 = np.sum(Y, 1)\n",
    "    s2 = np.sum(P, 1)\n",
    "    ss = np.sum(Y * P, 1)\n",
    "\n",
    "    return np.sum(s1 + s2 - 2 * ss) / (n * l)\n",
    "def one_error(Y, P, O):\n",
    "    n = (Y.shape[0] + O.shape[0]) // 2\n",
    "\n",
    "    i = np.argmax(O, 1)\n",
    "\n",
    "    return np.sum(1 - Y[range(n), i]) / n\n",
    "def ranking_loss(Y, P, O):\n",
    "    n = (Y.shape[0] + O.shape[0]) // 2\n",
    "    l = (Y.shape[1] + O.shape[1]) // 2\n",
    "\n",
    "    p = np.zeros(n)\n",
    "    q = np.sum(Y, 1)\n",
    "\n",
    "    r, c = np.nonzero(Y)\n",
    "    for i, j in zip(r, c): \n",
    "        p[i] += np.sum((Y[i, : ] < 0.5) * (O[i, : ] >= O[i, j]))\n",
    "\n",
    "    i = (q > 0) * (q < l)\n",
    "\n",
    "    return np.sum(p[i] / (q[i] * (l - q[i]))) / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4d2b8e",
   "metadata": {},
   "source": [
    "# Sampling of LSDMLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b7f98f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T08:11:53.476944Z",
     "start_time": "2024-03-12T08:11:53.450013Z"
    },
    "code_folding": [
     0,
     17,
     21,
     24
    ]
   },
   "outputs": [],
   "source": [
    "def Distance(df,weights,k,p1):\n",
    "    n = df.shape[0]  \n",
    "    distances = np.zeros((n, n))  \n",
    "    data=np.array(df)\n",
    "    if p1==np.inf:\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                distances[i, j]=np.max(weights* np.abs(data[i] - data[j]))               \n",
    "    else:\n",
    "        for i in range(n):\n",
    "            for j in range(i, n): \n",
    "                distances[i, j] = np.power(np.sum(weights * np.abs(data[i] - data[j])**p1), 1/p1)\n",
    "        distances = distances + distances.T - np.diag(distances.diagonal())\n",
    "    sorted_indices = np.argsort(distances, axis=1)\n",
    "    sorted_indices = sorted_indices[:, ::1]\n",
    "    topk_indices= sorted_indices[:, :k]\n",
    "    return topk_indices\n",
    "def assign_weights(arr):\n",
    "    arr =np.abs(arr)\n",
    "    weights = np.exp(arr - np.max(arr)) / np.sum(np.exp(arr - np.max(arr)))\n",
    "    return weights\n",
    "def label_assign_distance(v1, v2, w):\n",
    "    distance = np.power(np.sum(w * np.abs(v1 - v2)**2), 1/2)\n",
    "    return distance\n",
    "def LSDMLO(df1, df2, W, sp):\n",
    "    ImrMatrix=ImR(X,y)\n",
    "    n_neighbors = 5\n",
    "    p=2\n",
    "    cos_sim = label_similarity(np.array(df2),10)\n",
    "    non_zero_indices = [np.where(row != 0)[0].tolist() for row in cos_sim]\n",
    "    row_sums = cos_sim.sum(axis=1)\n",
    "    normalized_label_weight = cos_sim / row_sums[:, np.newaxis]\n",
    "    MinLabeldic, MajLabeldic = Labeltype(np.array(df1), np.array(df2))\n",
    "    ImbalanceRatioMatrix, MeanIR, countmatrix = Imbalance(np.array(df1), np.array(df2))\n",
    "    MinLabelindex = list(MinLabeldic.keys())\n",
    "    C = np.zeros((df1.shape[0], len(MinLabelindex)))\n",
    "    C_hat=np.zeros((df1.shape[0], len(MinLabelindex))) \n",
    "    indices_dict = {}\n",
    "    for tail_label in MinLabelindex:\n",
    "        all_relevant=non_zero_indices[tail_label]\n",
    "        sub_index = np.where(df2[MinLabeldic[tail_label]] == 1)[0]\n",
    "        idx = MinLabelindex.index(tail_label)      \n",
    "        W_tail_label = W[:, tail_label]\n",
    "        sorted_indices = np.argsort(W_tail_label)\n",
    "        sorted_column = W_tail_label[sorted_indices]\n",
    "        featureWeight = assign_weights(W_tail_label)\n",
    "        indices = Distance(df1, featureWeight, n_neighbors + 1, p)\n",
    "        indices_dict[tail_label] = indices\n",
    "        for i in range(df1.shape[0]):\n",
    "            if df2.iloc[i,tail_label]==0:\n",
    "                continue\n",
    "            count=0\n",
    "            for j in indices[i, 1:]:\n",
    "                if df2.iloc[i,tail_label]==df2.iloc[j,tail_label]:\n",
    "                    count +=1\n",
    "            C[i, MinLabelindex.index(tail_label)] = count / n_neighbors\n",
    "            count1list=[]\n",
    "            if all_relevant:   \n",
    "                for k in all_relevant:\n",
    "                    count1=0\n",
    "                    for j in indices[i, :]:\n",
    "                        if df2.iloc[j, k] == 1:\n",
    "                            count1+= 1\n",
    "                    count1list.append(count1)\n",
    "                C_hat[i, MinLabelindex.index(tail_label)]=max(count1list) / n_neighbors      \n",
    "    Ins_Weight=np.zeros(df1.shape[0])  \n",
    "    tem = np.zeros([df1.shape[0], len(MinLabelindex)])\n",
    "    tem_hat = np.zeros([df1.shape[0], len(MinLabelindex)])\n",
    "    for j in range(len(MinLabelindex)):\n",
    "        SumC = 0.0\n",
    "        sum_C_1 = 0.0\n",
    "        c = 0\n",
    "        c_1 = 0\n",
    "\n",
    "        for i in range(df1.shape[0]):\n",
    "            if C[i, j] < 1 and C[i, j] != 0:\n",
    "                SumC += C[i, j]\n",
    "                c += 1\n",
    "            if C_hat[i, j] != 0:\n",
    "                sum_C_1 += C_hat[i, j]\n",
    "                c_1 += 1\n",
    "\n",
    "            if SumC != 0.0 and c != 0:\n",
    "                if C[i, j] < 1 and C[i, j] != 0:\n",
    "                    tem[i, j] = C[i, j] / SumC\n",
    "            else:\n",
    "                tem[i, j] = 0\n",
    "\n",
    "            if sum_C_1 != 0.0 and c_1 != 0:\n",
    "                if C_hat[i, j] != 0:\n",
    "                    tem_hat[i, j] = C_hat[i, j] / sum_C_1\n",
    "            else:\n",
    "                tem_hat[i, j] = 0\n",
    "    SumW = 0\n",
    "    for i in range(df1.shape[0]):\n",
    "        for j in range(len(MinLabelindex)):\n",
    "            if tem[i, j] != 0:\n",
    "                Ins_Weight[i] += tem[i, j] + tem_hat[i, j]\n",
    "        SumW += Ins_Weight[i]\n",
    "    non_zero_elements = []\n",
    "\n",
    "    for row in tem:\n",
    "        for element in row:\n",
    "            if element != 0:\n",
    "                non_zero_elements.append(element)\n",
    "                \n",
    "           \n",
    "    n_sample = int(df1.shape[0] * sp)\n",
    "    new_X = np.zeros((n_sample, df1.shape[1]))\n",
    "    target = np.zeros((n_sample, df2.shape[1]))\n",
    "    count = 0 \n",
    "    \n",
    "    while count < n_sample:\n",
    "        random_count=np.random.random()*SumW\n",
    "        seed=0\n",
    "        s=0\n",
    "        for k in range(len(Ins_Weight)):\n",
    "            s+=Ins_Weight[k]\n",
    "            if(random_count<=s):\n",
    "                seed=k\n",
    "                break    \n",
    "        seedtype = np.where(np.array(df2.iloc[seed]) == 1)[0]\n",
    "        set1 = set(seedtype)\n",
    "        set2 = set(MinLabelindex)\n",
    "        intersection = set1.intersection(set2)\n",
    "        intersection_list = list(intersection)\n",
    "        if not intersection_list:\n",
    "            continue  # 当 intersection_list 为空时，跳过当前循环\n",
    "        select_index=np.random.choice(intersection_list)\n",
    "        reference = np.random.choice(indices_dict[select_index][seed, 1:])\n",
    "        all_point = indices_dict[select_index][seed, :]\n",
    "        nn_df = df2[df2.index.isin(all_point)]\n",
    "        ser = nn_df.sum(axis = 0, skipna = True)   \n",
    "        for j in range(df1.shape[1]):\n",
    "            ratio = np.random.random()\n",
    "            if feature_names[j][1] == 'NUMERIC':\n",
    "                new_X[count, j] = df1.iloc[seed, j] + ratio * (df1.iloc[reference, j] - df1.iloc[seed, j])\n",
    "            elif feature_names[j][1] == ['YES', 'NO'] or feature_names[j][1] == ['0', '1']:\n",
    "                rmd = np.random.choice([True, False])\n",
    "                if rmd:\n",
    "                    new_X[count, j] = df1.iloc[seed, j]\n",
    "                else:\n",
    "                    new_X[count, j] = df1.iloc[reference, j]\n",
    "            else:\n",
    "                new_X[count, j] = df1.iloc[seed, j]    \n",
    "        for j in range(df2.shape[1]):\n",
    "            if df2.iloc[seed, j] == df2.iloc[reference, j]:\n",
    "                target[count, j]=df2.iloc[seed, j]\n",
    "            else:\n",
    "                featureWeight = assign_weights(W[:, j])\n",
    "                distance1 = label_assign_distance(np.array(df1.iloc[seed, :]), new_X[count, :], featureWeight)\n",
    "                distance2 = label_assign_distance(np.array(df1.iloc[reference, :]), new_X[count, :], featureWeight)\n",
    "                if distance1 <= distance2:\n",
    "                    target[count, j] = df2.iloc[seed, j]\n",
    "                else:\n",
    "                    target[count, j] = df2.iloc[reference, j]\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    new_X = pd.DataFrame(new_X, columns=[x[0] for x in feature_names])\n",
    "    target = pd.DataFrame(target, columns=[y[0] for y in label_names])\n",
    "    LSDMLO_new_X = pd.concat([df1, new_X], axis=0).reset_index(drop=True)\n",
    "    LSDMLO_target = pd.concat([df2, target], axis=0).reset_index(drop=True)\n",
    "    return LSDMLO_new_X,LSDMLO_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fce568",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5c2c083",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T08:24:26.724242Z",
     "start_time": "2024-03-12T08:24:26.711296Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# traning\n",
    "# Only the call to MLkNN is provided here. You can add other methods and control them through c_idx\n",
    "def training(c_idx,sp,optmParameter):\n",
    "    Randomlist=[7,20,50,21,72]\n",
    "    Macro_F=[]\n",
    "    Micro_F=[]\n",
    "    Hamming_loss=[]\n",
    "    Ranking_loss=[]\n",
    "    One_error=[]\n",
    "    Macro_AUC=[]\n",
    "# 2-FOLD\n",
    "    for i in Randomlist:  \n",
    "        k_fold = IterativeStratification(n_splits=2,order=1,random_state=42)\n",
    "        j=0\n",
    "        for train,test in k_fold.split(X,y):\n",
    "            classifier =MLkNN(k=10)\n",
    "            dfx=pd.DataFrame(X[train],columns=[x[0] for x in feature_names])\n",
    "            dfy=pd.DataFrame(y[train],columns=[x[0] for x in label_names])\n",
    "            X1, y1 = X[train], y[train]\n",
    "            W = GetW(X1, y1, optmParameter)  # 修正后的代码行\n",
    "            new_X, new_y = LSDMLO(dfx, dfy, W, sp)\n",
    "            X1,y1=np.array(new_X),np.array(new_y)\n",
    "            classifier.fit(X1,y1)\n",
    "            X2,y2=np.array(X[test]),np.array(y[test])\n",
    "            ypred = classifier.predict(X2)\n",
    "            if scipy.sparse.issparse(ypred):\n",
    "                ypred = ypred.toarray()\n",
    "            yprob = classifier.predict_proba(X2)\n",
    "            if scipy.sparse.issparse(yprob):\n",
    "                yprob = yprob.toarray()\n",
    "            Macro_F.append(metrics.f1_score(y2, ypred,average='macro'))\n",
    "            Micro_F.append(metrics.f1_score(y2, ypred,average='micro'))\n",
    "            Ranking_loss.append(ranking_loss(y2, ypred, yprob))                     \n",
    "            Macro_AUC.append(macro_averaging_auc(y2, ypred, yprob))  \n",
    "            Hamming_loss.append(metrics.hamming_loss(y2, ypred)) \n",
    "            One_error.append(one_error(y2, ypred, yprob))\n",
    "\n",
    "    means = np.array([\n",
    "    np.mean(Macro_F),\n",
    "    np.mean(Micro_F),\n",
    "    np.mean(Macro_AUC),\n",
    "    np.mean(Ranking_loss),\n",
    "    np.mean(Hamming_loss),\n",
    "    np.mean(One_error)\n",
    "    ])\n",
    "\n",
    "    stds = np.array([\n",
    "        np.std(Macro_F),\n",
    "        np.std(Micro_F),\n",
    "        np.std(Macro_AUC),\n",
    "        np.std(Ranking_loss),\n",
    "        np.std(Hamming_loss),\n",
    "        np.std(One_error)\n",
    "    ])\n",
    "    rounded_means = np.round(means, 4)\n",
    "    rounded_stds = np.round(stds, 4)\n",
    "    print(tuple(rounded_means) + tuple(rounded_stds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d74497",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a4251af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T08:26:46.450103Z",
     "start_time": "2024-03-12T08:24:28.054152Z"
    },
    "code_folding": [
     7
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifierid: 1\n",
      "'alpha': 1,'beta': 0.0625,'spratio':0.01\n",
      "(0.4357, 0.4935, 0.7833, 0.2663, 0.2697, 0.3821, 0.0196, 0.015, 0.0053, 0.0038, 0.0094, 0.0075)\n",
      "'alpha': 1,'beta': 0.0625,'spratio':0.02\n",
      "(0.435, 0.4937, 0.7821, 0.2662, 0.2695, 0.3872, 0.0211, 0.0135, 0.0052, 0.0045, 0.0087, 0.0099)\n",
      "'alpha': 1,'beta': 0.0625,'spratio':0.05\n",
      "(0.44, 0.494, 0.7805, 0.2688, 0.2695, 0.3905, 0.0203, 0.0169, 0.0059, 0.0042, 0.0062, 0.0066)\n",
      "'alpha': 1,'beta': 0.0625,'spratio':0.1\n",
      "(0.4537, 0.4989, 0.7779, 0.2682, 0.2747, 0.3933, 0.0341, 0.0304, 0.006, 0.0063, 0.0094, 0.0146)\n",
      "'alpha': 1,'beta': 0.0625,'spratio':0.15\n",
      "(0.4565, 0.5027, 0.7737, 0.2719, 0.2795, 0.4071, 0.0229, 0.0198, 0.0074, 0.0082, 0.0099, 0.0117)\n",
      "'alpha': 1,'beta': 0.0625,'spratio':0.3\n",
      "(0.4536, 0.4971, 0.7699, 0.2722, 0.2805, 0.4125, 0.0265, 0.0198, 0.0082, 0.0057, 0.0101, 0.0199)\n",
      "'alpha': 1,'beta': 0.0625,'spratio':0.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2999117/707874921.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_specific_sampling\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf\"'alpha': {j[0]},'beta': {j[1]},'spratio':{sp}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptmParameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"----------------------------------------------------------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2999117/2393966391.py\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(c_idx, sp, optmParameter)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGetW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptmParameter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 修正后的代码行\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mnew_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSDMLO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2999117/3195018363.py\u001b[0m in \u001b[0;36mLSDMLO\u001b[0;34m(df1, df2, W, sp)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0msorted_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW_tail_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msorted_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mfeatureWeight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_tail_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatureWeight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mindices_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtail_label\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2999117/3195018363.py\u001b[0m in \u001b[0;36mDistance\u001b[0;34m(df, weights, k, p1)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mdistances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistances\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiagonal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0msorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 'alpha'=lamda1,'beta'=lamda2,not give all parameter combinations\n",
    "paracombine=[[2**(-2),2**(-6)],[2**(-2),2**(-4)],[2**(-1),2**(-3)],[2**(0),2**(-4)],[2**(0),2**(-2)],[2**(1),2**(-2)],[2**(2),2**(0)],[2**(3),2**(0)],[2**(3),2**(1)],[2**(4),2**(0)],[2**(4),2**(1)]]\n",
    "\n",
    "path_to_arff_files = [\"emotions.arff\",\"rcv1subset1.arff\",\"Corel5k.arff\",\"yahoo-Business1\",\"yahoo-Arts1.arff\"]\n",
    "label_counts = [6,101,374,28,25]\n",
    "select_feature=[1,0.01,1,0.01,0.01]\n",
    "data_specific_sampling=[0.01,0.02,0.05,0.1,0.15,0.3,0.5,0.7]\n",
    "for i, path_to_arff_file in enumerate(path_to_arff_files):\n",
    "    X, y, feature_names, label_names = load_from_arff(\n",
    "        path_to_arff_file,\n",
    "        label_count=label_counts[i],\n",
    "        label_location=\"end\",\n",
    "        load_sparse=False,\n",
    "        return_attribute_definitions=True\n",
    "    )\n",
    "    X,feature_names=FeatureSelect(select_feature[i],X)\n",
    "    y,label_names=LabelSelect(y)\n",
    "    dataset_name = path_to_arff_file.split(\".\")[0] \n",
    "    for c_idx in range(1,2):\n",
    "        results_list=[]\n",
    "        print (f\"classifierid: {c_idx}\")\n",
    "        MacroF_list = []\n",
    "        MacroAUCROC_list = []\n",
    "        MacroAUCPR_list = []\n",
    "        RankLoss_list = []\n",
    "        res2_list = []\n",
    "        res4_list = []\n",
    "        res6_list = []\n",
    "        res3_list = []\n",
    "        for j in paracombine:\n",
    "            optmParameter = {\n",
    "                            'alpha':j[0], \n",
    "                            'beta': j[1],  \n",
    "                            'gamma': 0.001,  \n",
    "                            'lamda': 2**(1),  \n",
    "                            'lamda2': 2**(5), \n",
    "                            'maxIter': 100, \n",
    "                            'minimumLossMargin': 0.01, \n",
    "                            'bQuiet': 1\n",
    "                            }         \n",
    "            for sp in data_specific_sampling:\n",
    "                print (f\"'alpha': {j[0]},'beta': {j[1]},'spratio':{sp}\")\n",
    "                training(c_idx,sp,optmParameter)\n",
    "print(\"----------------------------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
